// src/main/Interview/InterviewSessionManager.jsx
import React, { useState, useRef, useEffect } from "react";
import MicRecorder from "./asset/Mic/MicRecorder";
import { nextQuestion } from "./api/interview";
import { requestSpeechToText } from "./api/stt";
import Timer from "./asset/Timer";

const PHASE = {
  IDLE: "idle",
  READY: "ready",
  TTS: "tts",
  WAITING: "wait",
  RECORDING: "recording",
  UPLOADING: "uploading",
  COMPLETE: "complete",
};

/**
 * Props:
 *  - sessionId: string
 *  - jobRole: string
 *  - waitTime: number
 *  - answerDuration: number
 *  - allowRetry: boolean
 *  - initialQuestion: { question: string; audio_url: string }
 *  - onStatusChange: (phase: string) => void
 *  - onTimeUpdate: (remaining: number) => void
 *  - onNewQuestion: (question: string) => void
 *  - onAnswerComplete: (userText: string) => void
 */
function InterviewSessionManager({
                                   sessionId,
                                   jobRole,
                                   waitTime = 3,
                                   answerDuration = 10,
                                   allowRetry = true,
                                   initialQuestion,
                                   onStatusChange,
                                   onTimeUpdate,
                                   onNewQuestion,
                                   onAnswerComplete,
                                 }) {
  // ì‹œìž‘ì€ TTS ë‹¨ê³„ë¡œ: initialQuestionì˜ audio_url ìž¬ìƒ
  const [phase, setPhase] = useState(PHASE.TTS);
  const [question, setQuestion] = useState(initialQuestion);
  const [remainingTime, setRemainingTime] = useState(0);
  const [sttResult, setSttResult] = useState(null);

  const timerRef = useRef(null);
  const recorderRef = useRef(null);
  const audioRef = useRef(null);

  // ðŸ”Š TTS(ì˜¤ë””ì˜¤) ìž¬ìƒ
  useEffect(() => {
    onStatusChange?.(phase);
    if (phase === PHASE.TTS && question?.audio_url) {
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }
      const audio = new Audio("http://localhost:8000" + question.audio_url);
      audioRef.current = audio;
      audio.onended = () => setPhase(PHASE.WAITING);
      audio.play().catch(() => setPhase(PHASE.WAITING));
    }
  }, [phase, question, onStatusChange]);

  // â±ï¸ WAITING & RECORDING íƒ€ì´ë¨¸ ê´€ë¦¬
  useEffect(() => {
    clearInterval(timerRef.current);

    if (phase === PHASE.WAITING) {
      setRemainingTime(waitTime);
      onTimeUpdate?.(waitTime);
      timerRef.current = setInterval(() => {
        setRemainingTime(prev => {
          onTimeUpdate?.(prev - 1);
          if (prev <= 1) {
            clearInterval(timerRef.current);
            setPhase(PHASE.RECORDING);
            return 0;
          }
          return prev - 1;
        });
      }, 1000);
    }

    if (phase === PHASE.RECORDING) {
      setRemainingTime(answerDuration);
      onTimeUpdate?.(answerDuration);
      recorderRef.current?.start?.();
      timerRef.current = setInterval(() => {
        setRemainingTime(prev => {
          onTimeUpdate?.(prev - 1);
          if (prev <= 1) {
            clearInterval(timerRef.current);
            setPhase(PHASE.UPLOADING);
            recorderRef.current?.stop();
            return 0;
          }
          return prev - 1;
        });
      }, 1000);
    }

    return () => clearInterval(timerRef.current);
  }, [phase, waitTime, answerDuration, onTimeUpdate]);

  // ðŸŽ¤ ë…¹ìŒ ì™„ë£Œ â†’ STT ìš”ì²­
  const handleRecordingComplete = async blob => {
    setPhase(PHASE.UPLOADING);
    try {
      const data = await requestSpeechToText(blob);
      setSttResult(data.text);
      setPhase(PHASE.COMPLETE);
    } catch (err) {
      console.error("STT ì˜¤ë¥˜:", err);
    }
  };

  // âœ… COMPLETE ë‹¨ê³„ â†’ nextQuestion ìš”ì²­ í›„ ë‹¤ìŒ TTS
  useEffect(() => {
    if (phase === PHASE.COMPLETE && sttResult) {
      onAnswerComplete?.(sttResult);
      (async () => {
        try {
          const res = await nextQuestion(sessionId, sttResult);
          const { question: q, audio_url, done } = res.data;
          if (done) {
            setQuestion({ question: q, audio_url, done: true });
          } else {
            setQuestion({ question: q, audio_url });
            onNewQuestion?.(q);
            setPhase(PHASE.TTS);
          }
        } catch (err) {
          console.error("next_question ì‹¤íŒ¨", err);
        }
      })();
      setSttResult(null);
    }
  }, [phase, sttResult, sessionId, onAnswerComplete, onNewQuestion]);

  // â†º ë‹¤ì‹œ ë‹µë³€í•˜ê¸°
  const handleRetry = () => {
    setRemainingTime(waitTime);
    setPhase(PHASE.WAITING);
  };

  return (
      <div className="interview-session">
        <MicRecorder
            ref={recorderRef}
            isRecording={phase === PHASE.RECORDING}
            onStop={handleRecordingComplete}
        />

        {phase === PHASE.WAITING && (
            <div className="timer-area">
              <Timer duration={remainingTime} autoStart label="ëŒ€ê¸°ì‹œê°„" />
              {allowRetry && (
                  <button className="replay-button" onClick={handleRetry}>
                    ë‹¤ì‹œ ë‹µë³€í•˜ê¸°
                  </button>
              )}
            </div>
        )}
      </div>
  );
}

export default InterviewSessionManager;
